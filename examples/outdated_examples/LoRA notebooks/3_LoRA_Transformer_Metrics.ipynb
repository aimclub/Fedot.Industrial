{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cc2GwnzFGrzX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "from time import time\n",
        "import typing\n",
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.functional import mse_loss, l1_loss, binary_cross_entropy, cross_entropy\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOZTXJBOIdXc",
        "outputId": "285ef5ac-268b-4b82-e0eb-25461f2856dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJVTLKdVaPtN",
        "outputId": "318ac427-5256-4dbb-97fe-b627e29a2f03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.17.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "LcfWGI1zGFtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <picture>\n",
        ">   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/light-theme/tip.svg\">\n",
        ">   <img alt=\"Tip\" src=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/light-theme/tip.svg\">\n",
        "> </picture><br>\n",
        ">\n",
        "> ### Helpers & middlewares to measure metrics"
      ],
      "metadata": {
        "id": "FgdvKTkyGJ52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FpsWrapper:\n",
        "    \"\"\" Decorator to calculate the frames per second of a function\n",
        "    \"\"\"\n",
        "    def __init__(self, func: typing.Callable):\n",
        "        self.func = func\n",
        "        self.fps_list = deque([], maxlen=100)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        start = time.time()\n",
        "        results = self.func(self.instance, *args, **kwargs)\n",
        "        self.fps_list.append(1 / (time.time() - start))\n",
        "        self.instance.fps = np.mean(self.fps_list)\n",
        "        return results\n",
        "\n",
        "    def __get__(self, instance, owner):\n",
        "        self.instance = instance\n",
        "        return self.__call__.__get__(instance, owner)\n",
        "\n",
        "\n",
        "class OnnxInferenceModel:\n",
        "    \"\"\" Base class for all inference models that use onnxruntime\n",
        "\n",
        "    Attributes:\n",
        "        model_path (str, optional): Path to the model folder. Defaults to \"\".\n",
        "        force_cpu (bool, optional): Force the model to run on CPU or GPU. Defaults to GPU.\n",
        "        default_model_name (str, optional): Default model name. Defaults to \"model.onnx\".\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str = \"\",\n",
        "        force_cpu: bool = False,\n",
        "        default_model_name: str = \"model.onnx\",\n",
        "        *args, **kwargs\n",
        "        ):\n",
        "        self.model_path = model_path.replace(\"\\\\\", \"/\")\n",
        "        self.force_cpu = force_cpu\n",
        "        self.default_model_name = default_model_name\n",
        "\n",
        "        # check if model path is a directory with os path\n",
        "        if os.path.isdir(self.model_path):\n",
        "            self.model_path = os.path.join(self.model_path, self.default_model_name)\n",
        "\n",
        "        if not os.path.exists(self.model_path):\n",
        "            raise Exception(f\"Model path ({self.model_path}) does not exist\")\n",
        "\n",
        "        providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"] if ort.get_device() == \"GPU\" and not force_cpu else [\"CPUExecutionProvider\"]\n",
        "\n",
        "        self.model = ort.InferenceSession(self.model_path, providers=providers)\n",
        "\n",
        "        self.metadata = {}\n",
        "        if self.model.get_modelmeta().custom_metadata_map:\n",
        "            # add metadata to self object\n",
        "            for key, value in self.model.get_modelmeta().custom_metadata_map.items():\n",
        "                try:\n",
        "                    new_value = eval(value) # in case the value is a list or dict\n",
        "                except:\n",
        "                    new_value = value\n",
        "                self.metadata[key] = new_value\n",
        "\n",
        "        # Update providers priority to only CPUExecutionProvider\n",
        "        if self.force_cpu:\n",
        "            self.model.set_providers([\"CPUExecutionProvider\"])\n",
        "\n",
        "        self.input_shapes = [meta.shape for meta in self.model.get_inputs()]\n",
        "        self.input_names = [meta.name for meta in self.model._inputs_meta]\n",
        "        self.output_names = [meta.name for meta in self.model._outputs_meta]\n",
        "\n",
        "    def predict(self, data: np.ndarray, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @FpsWrapper\n",
        "    def __call__(self, data: np.ndarray):\n",
        "        results = self.predict(data)\n",
        "        return results"
      ],
      "metadata": {
        "id": "U2UunWfcaSXG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PerformanceEvaluator:\n",
        "    def __init__(self, model, dataset, device=None, batch_size=32):\n",
        "        self.model = model.model if hasattr(model, 'model') else model\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Measured performance metrics\n",
        "        self.latency = None\n",
        "        self.throughput = None\n",
        "        self.model_size = None\n",
        "        self.target_metrics = None\n",
        "\n",
        "    def eval(self):\n",
        "\n",
        "        result = dict(\n",
        "            latency=self.measure_latency(),\n",
        "            throughput=self.measure_throughput(),\n",
        "            model_size=self.measure_model_size(),\n",
        "        )\n",
        "\n",
        "        self.report()\n",
        "        return result\n",
        "\n",
        "    def measure_latency(self, reps: int = 50):\n",
        "        timings = np.zeros((reps, 1))\n",
        "        if torch.cuda.is_available():\n",
        "            self.warm_up_cuda()\n",
        "        with torch.no_grad():\n",
        "            with tqdm(total=reps, desc='Measuring latency', unit='rep') as pbar:\n",
        "                for rep in range(reps):\n",
        "                    for inputs, _ in self.data_loader:\n",
        "                        start_time = time.time()\n",
        "                        _ = self.model(inputs.to(self.device))\n",
        "                        end_time = time.time()\n",
        "                        if torch.cuda.is_available():\n",
        "                            torch.cuda.synchronize()\n",
        "                        curr_time = (end_time - start_time) * 1000\n",
        "                        timings[rep] = curr_time / inputs.size(0)\n",
        "                        break\n",
        "                    pbar.update(1)\n",
        "        self.latency = round(np.mean(timings) / reps, 5)\n",
        "        return self.latency\n",
        "\n",
        "    def measure_throughput(self, batches: int = 5):\n",
        "        total_data_size = 0\n",
        "        start_time = time.time()\n",
        "        # measure for n batches\n",
        "        with torch.no_grad():\n",
        "            with tqdm(total=batches, desc='Measuring throughput', unit='batch') as pbar:\n",
        "                for inputs, _ in self.data_loader:\n",
        "                    inputs = inputs.to(self.device)\n",
        "                    if batches == 0:\n",
        "                        break\n",
        "                    total_data_size += inputs.size(0)\n",
        "                    _ = self.model(inputs)\n",
        "                    batches -= 1\n",
        "                    pbar.update(1)\n",
        "        if self.device == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        total_time = (time.time() - start_time) / 1000\n",
        "        self.throughput = round(total_data_size / total_time, 0)\n",
        "        return self.throughput\n",
        "\n",
        "    def measure_model_size(self):\n",
        "        if isinstance(self.model, OnnxInferenceModel): # ONNXInferenceModel\n",
        "            size_all_mb = round(self.model.size(), 3) / 1024 ** 2\n",
        "        else:\n",
        "            param_size = 0\n",
        "            for param in self.model.parameters():\n",
        "                param_size += param.nelement() * param.element_size()\n",
        "            buffer_size = 0\n",
        "            for buffer in self.model.buffers():\n",
        "                buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "            size_all_mb = (param_size + buffer_size) / 1024 ** 2\n",
        "        self.model_size = round(size_all_mb, 3)\n",
        "        return self.model_size\n",
        "\n",
        "    def warm_up_cuda(self, num_iterations=10):\n",
        "        \"\"\"Warm up CUDA by performing some dummy computations\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            for _ in range(num_iterations):\n",
        "                inputs, _ = next(iter(self.data_loader))\n",
        "                inputs = inputs.to(self.device)\n",
        "                _ = self.model(inputs)\n",
        "\n",
        "    def report(self):\n",
        "        print(f\"Latency: {self.latency} ms/sample with batch_size {self.batch_size}\")\n",
        "        print(f\"Throughput: {self.throughput} samples/s with batch_size {self.batch_size}\")\n",
        "        print(f\"Model size: {self.model_size} MB\")"
      ],
      "metadata": {
        "id": "PpvuISfyaVnP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = [\"year\", \"month\", \"day\", \"dec_year\", \"sn_value\", \"sn_error\", \"obs_num\", \"unused1\"]\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/SN_d_tot_V2.0.csv\",\n",
        "    sep=\";\",\n",
        "    header=None,\n",
        "    names=names,\n",
        "    na_values=[\"-1\"],\n",
        "    index_col=False\n",
        ")"
      ],
      "metadata": {
        "id": "6ep6i-h8Iiqp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "start_id = max(df[df[\"obs_num\"] == 0].index.tolist()) + 1\n",
        "df = df[start_id:].copy()\n",
        "df[\"sn_value\"] = df[\"sn_value\"].astype(float)\n",
        "df_train = df[df[\"year\"] < 2000]\n",
        "df_test = df[df[\"year\"] >= 2000]\n",
        "\n",
        "spots_train = df_train[\"sn_value\"].to_numpy().reshape(-1, 1)\n",
        "spots_test = df_test[\"sn_value\"].to_numpy().reshape(-1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "spots_train = scaler.fit_transform(spots_train).flatten().tolist()\n",
        "spots_test = scaler.transform(spots_test).flatten().tolist()"
      ],
      "metadata": {
        "id": "441lP-p0G3BF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence Data Preparation\n",
        "SEQUENCE_SIZE = 10\n",
        "\n",
        "def to_sequences(seq_size, obs):\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in range(len(obs) - seq_size):\n",
        "        window = obs[i:(i + seq_size)]\n",
        "        after_window = obs[i + seq_size]\n",
        "        x.append(window)\n",
        "        y.append(after_window)\n",
        "    return torch.tensor(x, dtype=torch.float32).view(-1, seq_size, 1), torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "x_train, y_train = to_sequences(SEQUENCE_SIZE, spots_train)\n",
        "x_test, y_test = to_sequences(SEQUENCE_SIZE, spots_test)\n",
        "\n",
        "# Setup data loaders for batch\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(x_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "qjC3WOWEG56D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <picture>\n",
        ">   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/light-theme/check.svg\">\n",
        ">   <img alt=\"Check\" src=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/dark-theme/check.svg\">\n",
        "> </picture><br>\n",
        ">\n",
        "> ## Transformer architecture No Lora"
      ],
      "metadata": {
        "id": "Az4_SUzHGhIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding for Transformer\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Model definition using Transformer\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.decoder = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.decoder(x[:, -1, :])\n",
        "        return x"
      ],
      "metadata": {
        "id": "bApCdbEZG-QP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerModel().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF99gy-XHIot",
        "outputId": "9bf40907-61f1-4295-d41b-4507d88b0b8c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "epochs = 1000\n",
        "early_stop_count = 0\n",
        "min_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        x_batch, y_batch = batch\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            x_batch, y_batch = batch\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "    val_loss = np.mean(val_losses)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < min_val_loss:\n",
        "        min_val_loss = val_loss\n",
        "        early_stop_count = 0\n",
        "    else:\n",
        "        early_stop_count += 1\n",
        "\n",
        "    if early_stop_count >= 5:\n",
        "        print(\"Early stopping!\")\n",
        "        break\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQJ6aFH4HImX",
        "outputId": "db5c7cb1-016e-444d-c761-1772d655a26d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Validation Loss: 0.0499\n",
            "Epoch 2/1000, Validation Loss: 0.0418\n",
            "Epoch 3/1000, Validation Loss: 0.0467\n",
            "Epoch 4/1000, Validation Loss: 0.0420\n",
            "Epoch 5/1000, Validation Loss: 0.0574\n",
            "Epoch 6/1000, Validation Loss: 0.0387\n",
            "Epoch 7/1000, Validation Loss: 0.0369\n",
            "Epoch 8/1000, Validation Loss: 0.0354\n",
            "Epoch 9/1000, Validation Loss: 0.0366\n",
            "Epoch 10/1000, Validation Loss: 0.0414\n",
            "Epoch 11/1000, Validation Loss: 0.0357\n",
            "Epoch 12/1000, Validation Loss: 0.0403\n",
            "Early stopping!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <picture>\n",
        ">   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/light-theme/example.svg\">\n",
        ">   <img alt=\"Example\" src=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/dark-theme/example.svg\">\n",
        "> </picture><br>\n",
        ">\n",
        "> ### Measure of metrics no lora metrics"
      ],
      "metadata": {
        "id": "awQzZQHGGpjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        x_batch, y_batch = batch\n",
        "        x_batch = x_batch.to(device)\n",
        "        outputs = model(x_batch)\n",
        "        predictions.extend(outputs.squeeze().tolist())\n",
        "\n",
        "rmse = np.sqrt(np.mean((scaler.inverse_transform(np.array(predictions).reshape(-1, 1)) - scaler.inverse_transform(y_test.numpy().reshape(-1, 1)))**2))\n",
        "print(f\"Score (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsMmiXG7HOFQ",
        "outputId": "52dc2a4f-9c0b-45b8-d0d8-03ffcf196a70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score (RMSE): 15.0597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pfev_no_lora = PerformanceEvaluator(\n",
        "    model=model,\n",
        "    dataset=train_dataset,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "pfev_no_lora.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiHBdiD-akto",
        "outputId": "2ce3dc70-320a-4c1b-f420-6923638b2be4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Measuring latency: 100%|██████████| 50/50 [00:00<00:00, 563.83rep/s]\n",
            "Measuring throughput: 100%|██████████| 5/5 [00:00<00:00, 480.56batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latency: 0.00085 ms/sample with batch_size 32\n",
            "Throughput: 9306328.0 samples/s with batch_size 32\n",
            "Model size: 3.366 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latency': 0.00085, 'throughput': 9306328.0, 'model_size': 3.366}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <picture>\n",
        ">   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/light-theme/solution.svg\">\n",
        ">   <img alt=\"Solution\" src=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/dark-theme/solution.svg\">\n",
        "> </picture><br>\n",
        ">\n",
        "> ### LoRALayer implentation (same from 2.5 Colab)"
      ],
      "metadata": {
        "id": "Pq7obPbjGzLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRALayer():\n",
        "    def __init__(\n",
        "        self,\n",
        "        r: int,\n",
        "        lora_alpha: int,\n",
        "        lora_dropout: float,\n",
        "        merge_weights: bool,\n",
        "    ):\n",
        "        self.r = r\n",
        "        self.lora_alpha = lora_alpha\n",
        "        # Optional dropout\n",
        "        if lora_dropout > 0.:\n",
        "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
        "        else:\n",
        "            self.lora_dropout = lambda x: x\n",
        "        # Mark the weight as unmerged\n",
        "        self.merged = False\n",
        "        self.merge_weights = merge_weights\n",
        "\n",
        "class LinearLora(nn.Linear, LoRALayer):\n",
        "    # LoRA implemented in a dense layer\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        r: int = 0,\n",
        "        lora_alpha: int = 1,\n",
        "        lora_dropout: float = 0.,\n",
        "        fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
        "        merge_weights: bool = True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
        "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
        "                           merge_weights=merge_weights)\n",
        "\n",
        "        self.fan_in_fan_out = fan_in_fan_out\n",
        "        # Actual trainable parameters\n",
        "        if r > 0:\n",
        "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
        "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
        "            self.scaling = self.lora_alpha / self.r\n",
        "            # Freezing the pre-trained weight matrix\n",
        "            self.weight.requires_grad = False\n",
        "        self.reset_parameters()\n",
        "        if fan_in_fan_out:\n",
        "            self.weight.data = self.weight.data.transpose(0, 1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.Linear.reset_parameters(self)\n",
        "        if hasattr(self, 'lora_A'):\n",
        "            # initialize B the same way as the default for nn.Linear and A to zero\n",
        "            # this is different than what is described in the paper but should not affect performance\n",
        "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "            nn.init.zeros_(self.lora_B)\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        def T(w):\n",
        "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
        "        nn.Linear.train(self, mode)\n",
        "        if mode:\n",
        "            if self.merge_weights and self.merged:\n",
        "                # Make sure that the weights are not merged\n",
        "                if self.r > 0:\n",
        "                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling\n",
        "                self.merged = False\n",
        "        else:\n",
        "            if self.merge_weights and not self.merged:\n",
        "                # Merge the weights and mark it\n",
        "                if self.r > 0:\n",
        "                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling\n",
        "                self.merged = True\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        def T(w):\n",
        "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
        "        if self.r > 0 and not self.merged:\n",
        "            result = F.linear(x, T(self.weight), bias=self.bias)\n",
        "            result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling\n",
        "            return result\n",
        "        else:\n",
        "            return F.linear(x, T(self.weight), bias=self.bias)"
      ],
      "metadata": {
        "id": "-JxjKtbPHUgG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "> <picture>\n",
        ">   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/light-theme/complete.svg\">\n",
        ">   <img alt=\"Complete\" src=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/dark-theme/complete.svg\">\n",
        "> </picture><br>\n",
        ">\n",
        "> ### TransformerLora with encoder and decoder Lora Layers"
      ],
      "metadata": {
        "id": "YgJxS1BMG7tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition using TransformerLora\n",
        "class TransformerLora(nn.Module):\n",
        "    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
        "        super(TransformerLora, self).__init__()\n",
        "\n",
        "        self.encoder = LinearLora(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.decoder = LinearLora(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.decoder(x[:, -1, :])\n",
        "        return x"
      ],
      "metadata": {
        "id": "GqAEqzM5Hxm7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lora = TransformerLora().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu-Bira9JLOq",
        "outputId": "0d9bb58b-c4d5-4f1b-e6ca-baeb9b534f82"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model_lora.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "epochs = 1000\n",
        "early_stop_count = 0\n",
        "min_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_lora.train()\n",
        "    for batch in train_loader:\n",
        "        x_batch, y_batch = batch\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_lora(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model_lora.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            x_batch, y_batch = batch\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model_lora(x_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "    val_loss = np.mean(val_losses)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < min_val_loss:\n",
        "        min_val_loss = val_loss\n",
        "        early_stop_count = 0\n",
        "    else:\n",
        "        early_stop_count += 1\n",
        "\n",
        "    if early_stop_count >= 5:\n",
        "        print(\"Early stopping!\")\n",
        "        break\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-aT4BqpJO7b",
        "outputId": "dcb7d7b3-236c-4dd4-c70e-6ec1ab9bd574"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Validation Loss: 0.0458\n",
            "Epoch 2/1000, Validation Loss: 0.0378\n",
            "Epoch 3/1000, Validation Loss: 0.0464\n",
            "Epoch 4/1000, Validation Loss: 0.0390\n",
            "Epoch 5/1000, Validation Loss: 0.0448\n",
            "Epoch 6/1000, Validation Loss: 0.0458\n",
            "Early stopping!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <picture>\n",
        ">   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/light-theme/example.svg\">\n",
        ">   <img alt=\"Example\" src=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/dark-theme/example.svg\">\n",
        "> </picture><br>\n",
        ">\n",
        "> ### Measure of metrics with lora metrics"
      ],
      "metadata": {
        "id": "rbbOs0DdHIxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model_lora.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        x_batch, y_batch = batch\n",
        "        x_batch = x_batch.to(device)\n",
        "        outputs = model_lora(x_batch)\n",
        "        predictions.extend(outputs.squeeze().tolist())\n",
        "\n",
        "rmse = np.sqrt(np.mean((scaler.inverse_transform(np.array(predictions).reshape(-1, 1)) - scaler.inverse_transform(y_test.numpy().reshape(-1, 1)))**2))\n",
        "print(f\"Score (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-Ha4o_hJjeL",
        "outputId": "fda43ce0-ba6b-4178-854c-15079522a2f2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score (RMSE): 15.4552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pfev_lora = PerformanceEvaluator(\n",
        "    model=model,\n",
        "    dataset=train_dataset,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "pfev_lora.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mU89JbUbx33",
        "outputId": "d29c2442-e069-4c0e-af3b-c245f3e2e28e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Measuring latency: 100%|██████████| 50/50 [00:00<00:00, 567.24rep/s]\n",
            "Measuring throughput: 100%|██████████| 5/5 [00:00<00:00, 404.15batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latency: 0.00084 ms/sample with batch_size 32\n",
            "Throughput: 9782205.0 samples/s with batch_size 32\n",
            "Model size: 3.366 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latency': 0.00084, 'throughput': 9782205.0, 'model_size': 3.366}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <picture>\n",
        ">   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/light-theme/info.svg\">\n",
        ">   <img alt=\"Info\" src=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/dark-theme/info.svg\">\n",
        "> </picture><br>\n",
        ">\n",
        "> #### Compare with Microsoft Lora Implementation same model"
      ],
      "metadata": {
        "id": "HGmnvKhtHKGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loralib\n",
        "import loralib as lora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDoib7-ScRf1",
        "outputId": "834d72c1-3d1f-43de-f2a9-a23cd2677619"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loralib\n",
            "  Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: loralib\n",
            "Successfully installed loralib-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_microsoft = TransformerModel().to(device)"
      ],
      "metadata": {
        "id": "m3Lp2eCjzThV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model_microsoft.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "epochs = 1000\n",
        "early_stop_count = 0\n",
        "min_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_microsoft.train()\n",
        "    for batch in train_loader:\n",
        "        x_batch, y_batch = batch\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_microsoft(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model_microsoft.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            x_batch, y_batch = batch\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model_microsoft(x_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "    val_loss = np.mean(val_losses)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < min_val_loss:\n",
        "        min_val_loss = val_loss\n",
        "        early_stop_count = 0\n",
        "    else:\n",
        "        early_stop_count += 1\n",
        "\n",
        "    if early_stop_count >= 5:\n",
        "        print(\"Early stopping!\")\n",
        "        break\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzTzcgALzYdz",
        "outputId": "e711e974-9206-49ea-a789-81b0d11002a8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Validation Loss: 0.0365\n",
            "Epoch 2/1000, Validation Loss: 0.0375\n",
            "Epoch 3/1000, Validation Loss: 0.0427\n",
            "Epoch 4/1000, Validation Loss: 0.0365\n",
            "Epoch 5/1000, Validation Loss: 0.0416\n",
            "Epoch 6/1000, Validation Loss: 0.0367\n",
            "Epoch 7/1000, Validation Loss: 0.0389\n",
            "Epoch 8/1000, Validation Loss: 0.0376\n",
            "Early stopping!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora.mark_only_lora_as_trainable(model_microsoft)"
      ],
      "metadata": {
        "id": "xU1Tl0L7z09d"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pfev_model_microsoft = PerformanceEvaluator(\n",
        "    model=model_microsoft,\n",
        "    dataset=train_dataset,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "pfev_model_microsoft.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu3F5vMnzJKp",
        "outputId": "d00110c2-6811-47d1-8e08-bf43c26a32d6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Measuring latency: 100%|██████████| 50/50 [00:00<00:00, 558.69rep/s]\n",
            "Measuring throughput: 100%|██████████| 5/5 [00:00<00:00, 448.08batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latency: 0.00085 ms/sample with batch_size 32\n",
            "Throughput: 9572622.0 samples/s with batch_size 32\n",
            "Model size: 3.366 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latency': 0.00085, 'throughput': 9572622.0, 'model_size': 3.366}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora.mark_only_lora_as_trainable(model_microsoft, bias='all')"
      ],
      "metadata": {
        "id": "yj9uuLB0z2Yn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pfev_model_microsoft_all = PerformanceEvaluator(\n",
        "    model=model_microsoft,\n",
        "    dataset=train_dataset,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "pfev_model_microsoft_all.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqAHM1R3z7od",
        "outputId": "97283bf7-bd6b-4c63-93a2-a82dc06ac727"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Measuring latency: 100%|██████████| 50/50 [00:00<00:00, 557.96rep/s]\n",
            "Measuring throughput: 100%|██████████| 5/5 [00:00<00:00, 479.18batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latency: 0.00085 ms/sample with batch_size 32\n",
            "Throughput: 9463282.0 samples/s with batch_size 32\n",
            "Model size: 3.366 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latency': 0.00085, 'throughput': 9463282.0, 'model_size': 3.366}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora.mark_only_lora_as_trainable(model_microsoft, bias='lora_only')"
      ],
      "metadata": {
        "id": "L7zyEx2J0EXH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pfev_model_microsoft_lora = PerformanceEvaluator(\n",
        "    model=model_microsoft,\n",
        "    dataset=train_dataset,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "pfev_model_microsoft_lora.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvZ1UaWL0GlV",
        "outputId": "e8d6afac-8aa8-4c21-8964-ed79f1326c1f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Measuring latency: 100%|██████████| 50/50 [00:00<00:00, 566.65rep/s]\n",
            "Measuring throughput: 100%|██████████| 5/5 [00:00<00:00, 476.41batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latency: 0.00084 ms/sample with batch_size 32\n",
            "Throughput: 8586088.0 samples/s with batch_size 32\n",
            "Model size: 3.366 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latency': 0.00084, 'throughput': 8586088.0, 'model_size': 3.366}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <picture>\n",
        ">   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/light-theme/success.svg\">\n",
        ">   <img alt=\"Success\" src=\"https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/dark-theme/success.svg\">\n",
        "> </picture><br>\n",
        ">\n",
        "> # Results\n",
        "\n",
        "```\n",
        "# Before\n",
        "{'latency': 0.00085, 'throughput': 9306328.0, 'model_size': 3.366}\n",
        "# Own Implementation\n",
        "{'latency': 0.00084, 'throughput': 9782205.0, 'model_size': 3.366}\n",
        "# Microsoft implementation default\n",
        "{'latency': 0.00085, 'throughput': 9572622.0, 'model_size': 3.366}\n",
        "# Microsoft implementation all\n",
        "{'latency': 0.00085, 'throughput': 9463282.0, 'model_size': 3.366}\n",
        "# Microsoft implementation lora_only\n",
        "{'latency': 0.00084, 'throughput': 8586088.0, 'model_size': 3.366}\n",
        "```\n"
      ],
      "metadata": {
        "id": "9tHtChQOHS9l"
      }
    }
  ]
}